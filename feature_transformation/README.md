# Feature transformation #


## Table of contents

* [Introduction](#introduction)
* [Requirements](#requirements)
* [Build](#build)
* [Example use cases](#example-use-cases)

## Introduction

Experiment transform numerical and categorical features to higher-dimension vector.

This source code based on implementation of FastText written in *C++*. With *concept_definition.json* and *data_train_xxx.csv*, the output model transforms <itemid, value> pair to vector.


## Requirements

Generally, similar to **fastText** building on modern Mac OS and Linux distributions, **feature2vec** requires a compiler with good C++11 support, including:

* (g++-4.7.2 or newer) or (clang-3.3 or newer)

```
sudo apt-get update
sudo apt-get install g++
```

Compilation is carried out using a Makefile, so you will need to have a working **make**.

For more detail, please read at [Requirements of FastText](https://github.com/facebookresearch/fastText/blob/master/README.md#requirements)

**Packages specilized for Feature2Vec:**

*  Json in C++:

```
# Linux
$ sudo apt-get install libjsoncpp-dev

# MAC
$ brew install jsoncpp
```

## Build

```
$ cd feature_transformation
$ make
```

## Example use cases

### Run test case

```
$ ./feature2vec.exe test -dict ../test_data/concept_definition_test.json -input ../test_data/sample_train.csv -output aaa
```

### Feature transform

#### 1. Prepare data:

There are two files we need to prepare for training **feature2vec**: `concept_definition.json` and `data_train.csv`. Both of them are generated by `Python` code. For more detail, read `preprocessing/README.md`

#### 2. Train features

* Calculate the average number of events happening at the same time of each admission:

```
WITH A AS (select hadm_id, charttime, count(1) as countEvents from chartevents group by hadm_id, charttime)
SELECT MIN(countEvents), MAX(countEvents)
    , percentile_disc(0.25) WITHIN GROUP (ORDER BY countEvents)
    , percentile_disc(0.5) WITHIN GROUP (ORDER BY countEvents)
    , percentile_disc(0.75) WITHIN GROUP (ORDER BY countEvents)
FROM A
```

Here is the result:

| min | max | percentile_25th | percentile_50th | percentile_75th |
|-----|-----|-----------------|-----------------|-----------------|
|   1 | 391 |               7 |              15 |              30 |


##### 2.1 Using Skip-MF

```
$ ./feature2vec.exe skipgram -dict ../data/concept_definition.json -input ../data/data_train_chartevents_raw.csv -output ../models/skipgram_ce_nm -epoch 5 -verbose 2

Number of parsed feature definitions: 6380
Number of feature values: 189163
Number of feature segments: 759229

Start at: 2018-09-06.10:40:52
Read 283M events
End at: 2018-09-06.14:01:52. Duration: ~3h 20 mins

#### TEST SAMPLE ####
./feature2vec.exe skipgram -dict ../data/concept_definition.json -input ../data/data_train_chartevents_raw.csv.0.5M -output ../models/skipgram_ce_nm_05M -epoch 5 -verbose 2

# CASE 1
$ sudo ./feature2vec.exe skipgram -dict ../data/concept_definition.json -input /media/tuanta/USB/hattt/data/data_train_chartevents_raw.csv -output /media/tuanta/USB/hattt/models/skipgram_ce_nm_ws_60_dim_100_epoch_1 -ws 60 -dim 100 -epoch 1 -verbose 2

# CASE 2
sudo ./feature2vec.exe skipgram -dict ../data/concept_definition.json -input /media/tuanta/USB/hattt/data/data_train_chartevents_raw.csv -output /media/tuanta/USB/hattt/models/skipgram_ce_nm_ws_180_dim_100_epoch_1 -ws 180 -dim 100 -epoch 1 -verbose 2

# CASE 3
sudo ./feature2vec.exe skipgram -dict ../data/concept_definition.json -input /media/tuanta/USB/hattt/data/data_train_chartevents_raw.csv -output /media/tuanta/USB/hattt/models/skipgram_ce_nm_ws_360_dim_100_epoch_1 -ws 360 -dim 100 -epoch 1 -verbose 2

```
I have 2 different servers: (1) Server_1: 16G RAM, 8 cores and (2) Server_2: 32G RAM, 16 cores. It took **15 mins** to finish `readFromFile()` on Server_1 and **66 mins** on Server_2.

* CASE 1 `-ws 60 -dim 100 -epoch 1 -thread 12`: **4.25 hours** (2018-10-19.22:56:40 - 2018-10-20.03:14:48)
* CASE 2 `-ws 180 -dim 100 -epoch 1 -thread 12`: **~10 hours** (2018-10-20.08:19:07 - 2018-10-20.17:57:00)
* CASE 3 `-ws 360 -dim 100 -epoch 1 -thread 12`: **~15 hours** (2018-10-20.23:04:09 - )

##### 2.2 Using CBOMF

```
$ ./feature2vec.exe cbow -dict ../data/concept_definition.json -input ../data/data_train_chartevents_raw.csv.15percent -output ../models/cbow_ce_nm_ws_60_dim_100_epoch_1_test -ws 60 -dim 100 -epoch 1 -verbose 2

# CASE 4
$ ./feature2vec.exe cbow -dict ../data/concept_definition.json -input /media/tuanta/USB/hattt/data/data_train_chartevents_raw.csv -output /media/tuanta/USB/hattt/models/cbow_ce_nm_ws_60_dim_100_epoch_1 -ws 60 -dim 100 -epoch 1 -verbose 2

# CASE 5
sudo ./feature2vec.exe cbow -dict ../data/concept_definition.json -input /media/tuanta/USB/hattt/data/data_train_chartevents_raw.csv -output /media/tuanta/USB/hattt/models/cbow_ce_nm_ws_180_dim_100_epoch_1 -ws 180 -dim 100 -epoch 1 -verbose 2

# CASE 6
sudo ./feature2vec.exe cbow -dict ../data/concept_definition.json -input /media/tuanta/USB/hattt/data/data_train_chartevents_raw.csv -output /media/tuanta/USB/hattt/models/cbow_ce_nm_ws_360_dim_100_epoch_1 -ws 360 -dim 100 -epoch 1 -verbose 2
```

* CASE 4 `-ws 60 -dim 100 -epoch 1 -thread 12`: **~ hours**
* CASE 5 `-ws 180 -dim 200 -thread 12`: **~ hours**
* CASE 6 `-ws 360 -dim 300 -epoch 1 -thread 12`: **~ hours**


### Print feature vector
Print feature vector by itemid and its value

```
$ ./feature2vec.exe print-feature-vectors model_name.bin ../test_data/queries.txt ../output/queries_vector.txt
```