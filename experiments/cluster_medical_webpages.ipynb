{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/hatruong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/hatruong/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import nltk\n",
    "import unicodedata\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from fuzzywuzzy import fuzz\n",
    "import operator\n",
    "\n",
    "nltk.download('punkt') # for tokenize\n",
    "nltk.download('wordnet') # for stemming & lemmertize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Load content from file then parse it to dictionary\n",
    "\n",
    "    Args:\n",
    "        file_path (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "        temp = json.loads(text)\n",
    "        data = temp\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_concept_definition(file_path):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        file_path (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "\n",
    "    No Longer Raises:\n",
    "        e: Description\n",
    "    \"\"\"\n",
    "    concepts = dict()\n",
    "    data = load_dict_from_json(file_path)\n",
    "    for f in data['definition']:\n",
    "        conceptid = f['conceptid']\n",
    "        concepts[conceptid] = dict()\n",
    "        concepts[conceptid]['type'] = f['type']\n",
    "        concepts[conceptid]['min_value'] = f['min_value']\n",
    "        concepts[conceptid]['max_value'] = f['max_value']\n",
    "        concepts[conceptid]['multiply'] = f['multiply']\n",
    "        concepts[conceptid]['data'] = dict()\n",
    "        for v in f['data']:\n",
    "            concepts[conceptid]['data'][v['value']] = v['id']\n",
    "\n",
    "        concepts[conceptid]['segments'] = dict()\n",
    "        for s in f['segments']:\n",
    "            concepts[conceptid]['segments'][s['value']] = s['id']\n",
    "\n",
    "        concepts[conceptid]['hashmaps'] = dict()\n",
    "        for h in f['hashmaps']:\n",
    "            concepts[conceptid]['hashmaps'][h['value']] = h['hash']\n",
    "\n",
    "    item2concept = dict()\n",
    "    for c in data['item2concept']:\n",
    "        item2concept[c['itemid']] = c['conceptid']\n",
    "\n",
    "    return concepts, item2concept\n",
    "\n",
    "def load_d_items(d_items_fullpath):\n",
    "    df = pd.read_csv(d_items_fullpath)\n",
    "    temp = df.to_dict('records')\n",
    "    d_items_dict = dict()\n",
    "    for item in temp:\n",
    "        d_items_dict[item['itemid']] = item\n",
    "\n",
    "    return d_items_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_items_fullpath = '../data/d_items.csv'\n",
    "d_items_dict = load_d_items(d_items_fullpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total concepts: 6380\n",
      "Already crawled 74 concepts\n"
     ]
    }
   ],
   "source": [
    "concept_dir = '../data'\n",
    "export_dir = '../data/webpages'\n",
    "CONCEPT_WEBPAGES_FILE_NAME = 'concept_webpage.csv'\n",
    "CONCEPT_DEFINITION_FILENAME = 'concept_definition.json'\n",
    "\n",
    "# load crawled concept definition\n",
    "concept_fullpath = os.path.join(\n",
    "        concept_dir, CONCEPT_DEFINITION_FILENAME)\n",
    "concept_definitions, _ = load_concept_definition(concept_fullpath)\n",
    "print('Total concepts: %s' % len(concept_definitions))\n",
    "    \n",
    "# load crawled concept webpages\n",
    "concept_webpage_fullpath = os.path.join(\n",
    "        concept_dir, CONCEPT_WEBPAGES_FILE_NAME)\n",
    "\n",
    "concept_webpage_dict = dict()\n",
    "df = pd.read_csv(concept_webpage_fullpath)\n",
    "for item in df.to_dict('records'):\n",
    "    concept_webpage_dict[item['conceptid']] = item['encrypted_urls'].split(',')\n",
    "\n",
    "print('Already crawled %s concepts' % len(concept_webpage_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of carevue concepts: 50\n",
      "Number of nb_clusters: 50\n"
     ]
    }
   ],
   "source": [
    "# number of carevue concepts\n",
    "nb_cv_concept = len(\n",
    "    [idx for idx in concept_webpage_dict.keys() if idx <= 220000])\n",
    "print('Number of carevue concepts: %s' % nb_cv_concept)\n",
    "\n",
    "nb_clusters = nb_cv_concept\n",
    "if nb_cv_concept < len(concept_webpage_dict) / 2:\n",
    "    nb_clusters = len(concept_webpage_dict) - nb_cv_concept\n",
    "print('Number of nb_clusters: %s' % nb_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 3204\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "DATA_DIR = '../data/webpages'\n",
    "filenames = [f for f in os.listdir(DATA_DIR) if os.path.isfile(os.path.join(DATA_DIR, f))]\n",
    "print('Total files: %s' % len(filenames))\n",
    "# datafiles[0]\n",
    "data = dict()\n",
    "for fname in filenames:\n",
    "    with open(os.path.join(DATA_DIR, fname), 'r') as f:\n",
    "        content = f.read()\n",
    "        data[fname] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Replacement(Enum):\n",
    "\n",
    "    \"\"\"Define common constants\n",
    "\n",
    "    Attributes:\n",
    "        CURRENCY (str): Description\n",
    "        DATETIME (str): Description\n",
    "        EMAIL (str): Description\n",
    "        EMOJI_NEG (str): Description\n",
    "        EMOJI_POS (str): Description\n",
    "        NUMBER (str): Description\n",
    "        PHONE (str): Description\n",
    "        URL (str): Description\n",
    "\n",
    "    \"\"\"\n",
    "    EMAIL = ' '\n",
    "    URL = ' '\n",
    "    NUMBER = ' '\n",
    "    PHONE = ' '\n",
    "    CURRENCY = ' '\n",
    "    DATETIME = ' '\n",
    "    \n",
    "def handle_url(text):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    text = re.sub(r'http\\S+', Replacement.URL.value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def handle_email(text):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    return re.sub(r'(\\w+@\\w+)', Replacement.EMAIL.value, text)\n",
    "\n",
    "\n",
    "def handle_numbers(text):\n",
    "    \"\"\"Summary\n",
    "\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    # normal numbers\n",
    "    text = re.sub(r'^\\d+\\s|\\s\\d+\\s|\\s\\d+$', Replacement.NUMBER.value, text)\n",
    "    text = re.sub(r'\\b[\\d.\\/,]+', Replacement.NUMBER.value, text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def handle_phone(text):\n",
    "    \"\"\"\n",
    "    Handle cases:\n",
    "            XX XXX XXX\n",
    "            XXX XXX XXX\n",
    "            XXXXXXXXX\n",
    "        delimiter: whitespace OR - OR empty\n",
    "\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    return re.sub(r'([\\+\\s]*\\d{2,}[-\\s.]?\\d{3,4}[-\\s.]?\\d{3,4})',\n",
    "                  Replacement.PHONE.value, text)\n",
    "\n",
    "def remove_non_alphabet(text):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    text = re.sub(\n",
    "        r'[^a-zA-Z]', ' ', text\n",
    "    )\n",
    "    return text\n",
    "\n",
    "def handle_datetime(text):\n",
    "    \"\"\"\n",
    "    Handle cases: MM/YYYY, DD/MM/YYYY, DD/MM\n",
    "    delimiters: /.-\n",
    "\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    # MM/YYYY\n",
    "    group_1 = r'(\\d{1,2}[-./]\\d{4})'\n",
    "\n",
    "    # DD/MM or DD/MM/YYYY\n",
    "    group_2 = r'(\\d{1,2}[-./]\\d{1,2}([-./]\\d{4})?)'\n",
    "\n",
    "    # 09h56 OR 12h\n",
    "    group_3 = r'(\\d{1,2}(h|H)(\\d{1,2}(min|mins)?)?)'\n",
    "    return re.sub(r'(' + group_1 + '|' + group_2 + '|' + group_3 + ')',\n",
    "                  Replacement.DATETIME.value, text)\n",
    "\n",
    "def preprocess_sentence(text):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        text (TYPE): Description\n",
    "\n",
    "    Returns:\n",
    "        TYPE: Description\n",
    "    \"\"\"\n",
    "    funcs = [handle_url, handle_phone, handle_datetime, handle_numbers, handle_email,]\n",
    "    for f in funcs:\n",
    "#         logger.debug('preprocess %s' % str(f))\n",
    "        text = f(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "def contain_punctuation(word):\n",
    "    for p in string.punctuation:\n",
    "        if p in word:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def preprocess_document(document):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    document = document.replace('\\\\n', '\\n').strip('\"b')\n",
    "    document = unicodedata.normalize(\"NFKC\", document)\n",
    "\n",
    "    PUNCTUATIONS = string.punctuation + ' '\n",
    "    sents = [s for s in nltk.sent_tokenize(document) if len(s) > 1]\n",
    "#     print('\\nstep1: \\t', sents)\n",
    "\n",
    "    # preprocess each sentence\n",
    "    sents = [preprocess_sentence(s) for s in sents]\n",
    "#     print('\\nstep2: \\t', sents)\n",
    "\n",
    "    # filter again too short sentence\n",
    "    sents = [s.strip() for s in sents if len(s.strip()) > 1]\n",
    "#     print('\\nstep3: \\t', sents)\n",
    "\n",
    "    # tokenize words\n",
    "    words = [[w.strip(PUNCTUATIONS) for w in nltk.word_tokenize(s) if contain_punctuation(w) is False]\n",
    "             for s in sents]\n",
    "    \n",
    "    # stemming & lemming\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    words = [[stemmer.stem(w) for w in s] for s in words]\n",
    "    words = [[lemmatizer.lemmatize(w) for w in s] for s in words]\n",
    "    \n",
    "    # concat words to sentences\n",
    "    sents = [' '.join([w for w in s]) for s in words]\n",
    "    processed_document = ' '.join([s if s.endswith(tuple(string.punctuation))\n",
    "             else s + ' .' for s in sents])\n",
    "#     print('\\nstep4: \\t', sents)\n",
    "    return processed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'b\"Diastolic | definition of diastolic by Medical dictionary\\\\nhttps://medical-dictionary.thefreedictionary.com/diastolic\\\\npertaining to diastole, or the blood pressure at the instant of maximum cardiac relaxation.\\\\ndi\\\\xc2\\\\xb7a\\\\xc2\\\\xb7stol\\\\xc2\\\\xb7ic\\\\nRelating to diastole.\\\\ndiastolic\\\\nPertaining to DIASTOLE . The diastolic blood pressure is the pressure during diastole and is the lower of the two figures measured. The peak pressure is called the SYSTOLIC pressure.\\\\nDiastolic\\\\nThe phase of blood circulation in which the heart\\'s pumping chambers (ventricles) are being filled with blood. During this phase, the ventricles are at their most relaxed, and the pressure against the walls of the arteries is at its lowest.\\\\nWant to thank TFD for its existence? Tell a friend about us , add a link to this page, or visit the webmaster\\'s page for free fun content .\\\\nLink to this page:\\\\nReferences in periodicals archive ?\\\\nThe prolongation of IVRT more than 100 msec is a significant indicator of early LV diastolic dysfunction.\\\\nLEFT ATRIUM VOLUME AS A SURROGATE MARKER OF LEFT VENTRICULAR DIASTOLIC DYSFUNCTION\\\\nThe systolic and diastolic sizes of the left ventricle (end-systolic volume of the left ventricle, index of the end-systolic volume of the left ventricle, end-systolic dimension of the left ventricle, end-diastolic volume of the left ventricle, index of the end-diastolic volume of the left ventricle, end-diastolic dimension of the left ventricle) in patients in the first group (with no signs of CPH) were nearly similar to those of the control group.\\\\nDistinctive features of systolic function of the left ventricle in patients with chronic obstructive lung disease in extremely cold climate conditions\\\\n9] in their study on children aged 5-6 years, observed no gender difference in mean systolic BPs, but reported a mean diastolic BP of 97 (5) mmHg (higher than systolic) in the females.\\\\n\"'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = data[filenames[0]]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diastol definit of diastol by medic dictionari pertain to diastol or the blood pressur at the instant of maximum cardiac relax . relat to diastol . diastol pertain to diastol . the diastol blood pressur is the pressur dure diastol and is the lower of the two figur measur . the peak pressur is call the systol pressur . diastol the phase of blood circul in which the heart pump chamber ventricl are be fill with blood . dure this phase the ventricl are at their most relax and the pressur against the wall of the arteri is at it lowest . want to thank tfd for it exist . tell a friend about u add a link to this page or visit the webmast page for free fun content . link to this page refer in period archiv . the prolong of ivrt more than msec is a signific indic of earli lv diastol dysfunct . left atrium volum a a surrog marker of left ventricular diastol dysfunct the systol and diastol size of the left ventricl volum of the left ventricl index of the volum of the left ventricl dimens of the left ventricl volum of the left ventricl index of the volum of the left ventricl dimens of the left ventricl in patient in the first group with no sign of cph were near similar to those of the control group . distinct featur of systol function of the left ventricl in patient with chronic obstruct lung diseas in extrem cold climat condit in their studi on child age year observ no gender differ in mean systol bps but report a mean diastol bp of mmhg higher than systol in the femal .\n"
     ]
    }
   ],
   "source": [
    "# preprocess (sample for the first document)\n",
    "# replace \\n = line break\n",
    "new_doc = doc.replace('\\\\n', '\\n').strip('\"b')\n",
    "new_doc = unicodedata.normalize(\"NFKC\", new_doc)\n",
    "new_doc = preprocess_document(new_doc)\n",
    "print(new_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF vectorizer\n",
    "fnames = [fname for fname, content in data.items()]\n",
    "contents = [content for fname, content in data.items()]\n",
    "\n",
    "t0 = time()\n",
    "vectorizer = TfidfVectorizer(max_df=0.5, min_df=0.01, stop_words='english',\n",
    "                             use_idf=True, lowercase=True, preprocessor=preprocess_document)\n",
    "X = vectorizer.fit_transform(contents)\n",
    "print(\"done in %fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3204, 3528)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3204"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find similar concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_items_fullpath = '../data/d_items.csv'\n",
    "d_items_dict = load_d_items(d_items_fullpath)\n",
    "# actual_items_df = load_actual_items()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_sort_ratio('Skin'.lower(), 'Temp Skin [C]'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.token_set_ratio('Skin'.lower(), 'Temp Skin [C]'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio('Skin [Temperature]'.lower(), 'Temp Skin [C]'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_token_set_ratio('Skin'.lower(), 'Temp Skin [C]'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_token_sort_ratio('Skin'.lower(), 'Temp Skin [C]'.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_itemids = [idx for idx in concept_webpage_dict.keys() if idx <= 220000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Find similar items for item[211]=Heart Rate, is_number=True\n",
      "Top 5 similar items:\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(220045, 100, 'Heart Rate', 0),\n",
       " (3337, 76, 'Breath Rate', 0),\n",
       " (3494, 74, 'Lowest Heart Rate', 0),\n",
       " (517, 70, 'Pacer Rate', 0),\n",
       " (227470, 67, 'Sed Rate', 0)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concerned_id = 211 #cv_itemids[0]\n",
    "print('Find similar items for item[%s]=%s, is_number=%s' % (\n",
    "    concerned_id, d_items_dict[concerned_id]['label'], concept_definitions[concerned_id]['type'] == 0))\n",
    "label_scores = list()\n",
    "for compared_id in concept_definitions.keys():\n",
    "    if compared_id != concerned_id and compared_id in d_items_dict.keys():\n",
    "        # check the same: linksto (TODO), is_numeric (DONE)\n",
    "        if concept_definitions[compared_id]['type'] == concept_definitions[concerned_id]['type']:\n",
    "            s = fuzz.ratio(d_items_dict[concerned_id]['label'].lower(), d_items_dict[compared_id]['label'].lower())\n",
    "            label_scores.append((compared_id, s, d_items_dict[compared_id]['label'], \n",
    "                           concept_definitions[compared_id]['type']))\n",
    "label_scores = sorted(label_scores, key=operator.itemgetter(1), reverse=True)\n",
    "print('Top 5 similar items:\\n')\n",
    "label_scores[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = np.arange(-10, 10, 0.001)\n",
    "# Mean = 0, SD = 2.\n",
    "dist_a = stats.norm.pdf(x_axis,0,2)\n",
    "# Mean = 1, SD = 2\n",
    "dist_b = stats.norm.pdf(x_axis,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12499715452295898\n"
     ]
    }
   ],
   "source": [
    "print(stats.entropy(dist_a, dist_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ks_2samp\n",
    "import numpy as np\n",
    "np.random.seed(12345678)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.55370819 -1.45963199 -1.29458514 -1.50967395  1.5718749  -0.97569619\n",
      "  0.48069879  0.62561431  0.72235302  0.91032644]\n",
      "[ 0.50131113  1.88754276 -1.09088507 -0.6335349  -0.07426521  0.03976827\n",
      " -1.2169928  -0.30350157 -0.07096837  0.92335193]\n",
      "[ 0.55370819 -1.45963199 -1.29458514 -1.50967395  1.5718749  -0.97569619\n",
      "  0.48069879  0.62561431  0.72235302  0.91032644]\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "x = np.random.normal(0, 1, 1000)\n",
    "y = np.random.normal(0, 1, 1000)\n",
    "z = np.random.normal(1.1, 0.9, 1000)\n",
    "print(x[:10])\n",
    "print(y[:10])\n",
    "print(x[:10])\n",
    "print(type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x~y: pvalue = 95.18901680484964\n"
     ]
    }
   ],
   "source": [
    "print('x~y: pvalue =', ks_2samp(x, y)[1] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x~z: pvalue =  3.708149411924217e-75\n"
     ]
    }
   ],
   "source": [
    "print('x~z: pvalue = ', ks_2samp(x, z)[1] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "minibatch = True\n",
    "verbose = 2\n",
    "true_k = nb_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing dimensionality reduction using LSA\n",
      "done in 0.061733s\n",
      "Explained variance of the SVD step: 2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Performing dimensionality reduction using LSA\")\n",
    "t0 = time()\n",
    "# Vectorizer results are normalized, which makes KMeans behave as\n",
    "# spherical k-means for better results. Since LSA/SVD results are\n",
    "# not normalized, we have to redo the normalization.\n",
    "svd = TruncatedSVD(n_components)\n",
    "normalizer = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "X_reduce = lsa.fit_transform(X)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Explained variance of the SVD step: {}%\".format(\n",
    "    int(explained_variance * 100)))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3204, 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_reduce.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering sparse data with MiniBatchKMeans(batch_size=1000, compute_labels=True, init='k-means++',\n",
      "        init_size=1000, max_iter=100, max_no_improvement=10, n_clusters=50,\n",
      "        n_init=1, random_state=None, reassignment_ratio=0.01, tol=0.0,\n",
      "        verbose=2)\n",
      "Init 1/1 with method: k-means++\n",
      "Inertia for init 1/1: 0.113961\n",
      "Minibatch iteration 1/400: mean batch inertia: 0.000135, ewa inertia: 0.000135 \n",
      "Minibatch iteration 2/400: mean batch inertia: 0.000119, ewa inertia: 0.000125 \n",
      "Minibatch iteration 3/400: mean batch inertia: 0.000131, ewa inertia: 0.000129 \n",
      "Minibatch iteration 4/400: mean batch inertia: 0.000128, ewa inertia: 0.000128 \n",
      "Minibatch iteration 5/400: mean batch inertia: 0.000117, ewa inertia: 0.000121 \n",
      "Minibatch iteration 6/400: mean batch inertia: 0.000124, ewa inertia: 0.000123 \n",
      "Minibatch iteration 7/400: mean batch inertia: 0.000119, ewa inertia: 0.000120 \n",
      "Minibatch iteration 8/400: mean batch inertia: 0.000118, ewa inertia: 0.000119 \n",
      "Minibatch iteration 9/400: mean batch inertia: 0.000119, ewa inertia: 0.000119 \n",
      "Minibatch iteration 10/400: mean batch inertia: 0.000123, ewa inertia: 0.000122 \n",
      "Minibatch iteration 11/400: mean batch inertia: 0.000129, ewa inertia: 0.000126 \n",
      "Minibatch iteration 12/400: mean batch inertia: 0.000114, ewa inertia: 0.000119 \n",
      "Minibatch iteration 13/400: mean batch inertia: 0.000116, ewa inertia: 0.000117 \n",
      "Minibatch iteration 14/400: mean batch inertia: 0.000123, ewa inertia: 0.000121 \n",
      "Minibatch iteration 15/400: mean batch inertia: 0.000124, ewa inertia: 0.000123 \n",
      "Minibatch iteration 16/400: mean batch inertia: 0.000115, ewa inertia: 0.000118 \n",
      "Minibatch iteration 17/400: mean batch inertia: 0.000117, ewa inertia: 0.000117 \n",
      "Minibatch iteration 18/400: mean batch inertia: 0.000124, ewa inertia: 0.000121 \n",
      "Minibatch iteration 19/400: mean batch inertia: 0.000114, ewa inertia: 0.000117 \n",
      "Minibatch iteration 20/400: mean batch inertia: 0.000104, ewa inertia: 0.000109 \n",
      "Minibatch iteration 21/400: mean batch inertia: 0.000112, ewa inertia: 0.000111 \n",
      "Minibatch iteration 22/400: mean batch inertia: 0.000118, ewa inertia: 0.000115 \n",
      "Minibatch iteration 23/400: mean batch inertia: 0.000116, ewa inertia: 0.000116 \n",
      "Minibatch iteration 24/400: mean batch inertia: 0.000128, ewa inertia: 0.000123 \n",
      "Minibatch iteration 25/400: mean batch inertia: 0.000114, ewa inertia: 0.000117 \n",
      "Minibatch iteration 26/400: mean batch inertia: 0.000131, ewa inertia: 0.000126 \n",
      "Minibatch iteration 27/400: mean batch inertia: 0.000116, ewa inertia: 0.000120 \n",
      "Minibatch iteration 28/400: mean batch inertia: 0.000110, ewa inertia: 0.000114 \n",
      "Minibatch iteration 29/400: mean batch inertia: 0.000108, ewa inertia: 0.000111 \n",
      "Minibatch iteration 30/400: mean batch inertia: 0.000114, ewa inertia: 0.000113 \n",
      "Converged (lack of improvement in inertia) at iteration 30/400\n",
      "Computing label assignment and total inertia\n",
      "done in 0.103s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
    "                         init_size=1000, batch_size=1000, verbose=verbose)\n",
    "else:\n",
    "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
    "                verbose=verbose)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X_reduce)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Coefficient: 0.540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# #############################################################################\n",
    "# metric\n",
    "\n",
    "# print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "# print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "# print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "# print(\"Adjusted Rand-Index: %.3f\"\n",
    "#       % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "\n",
    "# The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. \n",
    "# Scores around zero indicate overlapping clusters.\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X_reduce, km.labels_, sample_size=1000))\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster 0: medic doctor health tube drug skin care seizur treatment infect\n",
      "Cluster 1: patient wa pressur ventil blood medic care tube effect time\n",
      "Cluster 2: ventil pressur patient volum wa lung breath airway increas blood\n",
      "Cluster 3: patient medic tube wa care doctor health drug skin treatment\n",
      "Cluster 4: ventil pressur patient volum lung breath airway peep respiratori wa\n",
      "Cluster 5: patient wa medic care tube blood doctor effect health drug\n",
      "Cluster 6: medic patient tube doctor health care drug skin wa treatment\n",
      "Cluster 7: zone expens explos explor explan explain expiratori expir expertis expert\n",
      "Cluster 8: patient pressur ventil wa blood volum increas breath care lung\n",
      "Cluster 9: patient medic wa tube care doctor blood health drug skin\n",
      "Cluster 10: patient ventil pressur wa volum breath lung blood increas respiratori\n",
      "Cluster 11: patient wa medic care blood tube effect doctor pressur treatment\n",
      "Cluster 12: ventil pressur patient volum lung breath airway peep respiratori tidal\n",
      "Cluster 13: ventil pressur patient volum lung breath airway peep wa respiratori\n",
      "Cluster 14: patient medic wa tube care doctor health drug skin blood\n",
      "Cluster 15: patient pressur ventil wa blood medic care tube time effect\n",
      "Cluster 16: medic tube doctor patient health care drug skin treatment seizur\n",
      "Cluster 17: medic patient tube care doctor health wa drug skin treatment\n",
      "Cluster 18: ventil pressur patient volum lung breath wa airway peep respiratori\n",
      "Cluster 19: patient medic wa tube care blood doctor health drug skin\n",
      "Cluster 20: ventil pressur patient volum lung breath airway peep respiratori tidal\n",
      "Cluster 21: patient wa medic pressur blood care tube effect treatment doctor\n",
      "Cluster 22: ventil patient pressur wa volum lung breath blood increas airway\n",
      "Cluster 23: patient medic tube care doctor wa health drug skin treatment\n",
      "Cluster 24: medic doctor health tube drug skin seizur care infect inform\n",
      "Cluster 25: patient wa medic care tube blood effect doctor drug treatment\n",
      "Cluster 26: patient pressur wa ventil blood medic care tube effect time\n",
      "Cluster 27: patient medic wa tube care doctor health drug skin blood\n",
      "Cluster 28: medic doctor tube health drug care skin seizur patient treatment\n",
      "Cluster 29: medic patient tube doctor care health drug skin wa treatment\n",
      "Cluster 30: patient ventil pressur wa blood volum breath lung increas care\n",
      "Cluster 31: ventil pressur patient volum lung breath wa airway peep respiratori\n",
      "Cluster 32: patient pressur ventil wa blood volum care increas medic time\n",
      "Cluster 33: patient ventil pressur wa volum blood breath lung increas studi\n",
      "Cluster 34: ventil pressur patient volum lung breath wa airway peep respiratori\n",
      "Cluster 35: medic patient tube doctor health care drug skin treatment wa\n",
      "Cluster 36: ventil pressur patient volum lung breath airway peep respiratori wa\n",
      "Cluster 37: patient medic wa tube care blood doctor health drug skin\n",
      "Cluster 38: patient wa pressur medic blood care tube ventil effect time\n",
      "Cluster 39: medic tube doctor health care drug skin patient seizur treatment\n",
      "Cluster 40: patient wa medic blood care tube pressur effect treatment doctor\n",
      "Cluster 41: ventil patient pressur wa volum lung breath blood increas airway\n",
      "Cluster 42: patient medic wa tube care doctor health blood drug skin\n",
      "Cluster 43: patient wa medic care tube blood doctor health drug skin\n",
      "Cluster 44: ventil pressur patient volum lung breath airway peep respiratori tidal\n",
      "Cluster 45: ventil pressur patient volum lung breath airway peep respiratori tidal\n",
      "Cluster 46: patient pressur ventil wa blood care medic volum tube increas\n",
      "Cluster 47: patient medic wa tube care blood doctor health drug skin\n",
      "Cluster 48: medic patient tube doctor care health drug skin wa treatment\n",
      "Cluster 49: ventil pressur patient volum wa lung breath airway respiratori peep\n"
     ]
    }
   ],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "if n_components:\n",
    "    original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "else:\n",
    "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(true_k):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({18: 32,\n",
       "         47: 76,\n",
       "         10: 39,\n",
       "         46: 68,\n",
       "         26: 65,\n",
       "         5: 92,\n",
       "         8: 61,\n",
       "         21: 82,\n",
       "         16: 63,\n",
       "         35: 64,\n",
       "         38: 76,\n",
       "         49: 29,\n",
       "         23: 143,\n",
       "         9: 87,\n",
       "         48: 87,\n",
       "         17: 111,\n",
       "         40: 67,\n",
       "         19: 104,\n",
       "         20: 34,\n",
       "         25: 85,\n",
       "         11: 84,\n",
       "         14: 171,\n",
       "         39: 70,\n",
       "         37: 81,\n",
       "         29: 100,\n",
       "         41: 43,\n",
       "         1: 72,\n",
       "         30: 45,\n",
       "         12: 35,\n",
       "         15: 67,\n",
       "         27: 141,\n",
       "         43: 91,\n",
       "         45: 27,\n",
       "         4: 31,\n",
       "         42: 90,\n",
       "         22: 43,\n",
       "         28: 27,\n",
       "         3: 155,\n",
       "         32: 46,\n",
       "         33: 55,\n",
       "         6: 63,\n",
       "         0: 12,\n",
       "         13: 38,\n",
       "         34: 34,\n",
       "         36: 33,\n",
       "         31: 22,\n",
       "         2: 25,\n",
       "         44: 13,\n",
       "         7: 7,\n",
       "         24: 18})"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0, 0, 0, 0, 0], \n",
    "              [0, 1, 0, 0, 0, 0], \n",
    "              [0, 0, 1, 0, 0, 0],\n",
    "              [0, 0, 0, 1, 0, 0],\n",
    "              [0, 0, 0, 0, 1, 0],\n",
    "              [0, 0, 0, 0, 0, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y):\n",
    "    return np.dot(x, y) / (np.sqrt(np.dot(x, x)) * np.sqrt(np.dot(y, y)))\n",
    "def euclidean_distance(x, y):   \n",
    "    return np.sqrt(np.sum((x - y) ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(X[0], X[1]))\n",
    "print(euclidean_distance(X[0], X[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity(X[0], X[2]))\n",
    "print(euclidean_distance(X[0], X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4142135623730951\n"
     ]
    }
   ],
   "source": [
    "print(euclidean_distance(X[1], X[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
